@article{davison2012automated,
  author = {Davison, Andrew},
  year = {2012},
  title = {Automated Capture of Experiment Context for Easier Reproducibility in Computational Research},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2012.41},
  url = {http://dx.doi.org/10.1109/mcse.2012.41},
  volume = {14},
  month = {12},
  pages = {48--56},
  number = {4}
}
@article{fomel2009guest,
  author = {Fomel, Sergey and Claerbout, Jon F.},
  year = {2009},
  title = {Guest Editors' Introduction: Reproducible Research},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2009.14},
  url = {http://dx.doi.org/10.1109/mcse.2009.14},
  volume = {11},
  month = {9},
  pages = {5--7},
  number = {1},
  abstract = {The articles in this special issue provide independent solutions for practical reproducible research systems. The use of Matlab-based tools such as the famous Wavelab and Sparselab packages in promoting reproducible research in computational harmonic analysis has been presented. In particular, the authors point to the success of the reproducible research discipline in increasing the reliability of computational research and reflect on the effort necessary for implementing this discipline in a research group and overcoming possible objections to it. An article also describes a Python interface to the well-known Clawpack package for solving hyperbolic partial differential equations that appear in wave propagation problems. The author argues strongly in favor of reproducible computations and shows an example using a simplified Python interface to Fortran code. An article also represents the field of bioinformatics, which has been a stronghold of reproducible research. It describes the cacher package, which is built on top of the R computing environment. Cacher enables a modular approach to reproducible computations by storing results of intermediate computations in a database. The special issue ends with an article on the legal aspects of reproducible research, including copyright and licensing issues.}
}
@book{vasilescu2015quality,
  author = {Vasilescu, Bogdan and Yu, Yue and Wang, Huaimin and Devanbu, Premkumar and Filkov, Vladimir},
  year = {2015},
  title = {Quality and productivity outcomes relating to continuous integration in GitHub},
  doi = {10.1145/2786805.2786850},
  url = {http://dx.doi.org/10.1145/2786805.2786850},
  pages = {805--816},
  abstract = {Software processes comprise many steps; coding is followed by building, integration testing, system testing, deployment, operations, among others. Software process integration and automation have been areas of key concern in software engineering, ever since the pioneering work of Osterweil; market pressures for Agility, and open, decentralized, software development have provided additional pressures for progress in this area. But do these innovations actually help projects? Given the numerous confounding factors that can influence project performance, it can be a challenge to discern the effects of process integration and automation. Software project ecosystems such as GitHub provide a new opportunity in this regard: one can readily find large numbers of projects in various stages of process integration and automation, and gather data on various influencing factors as well as productivity and quality outcomes. In this paper we use large, historical data on process metrics and outcomes in GitHub projects to discern the effects of one specific innovation in process automation: continuous integration. Our main finding is that continuous integration improves the productivity of project teams, who can integrate more outside contributions, without an observable diminishment in code quality.}
}
@article{wilson2014best,
  author = {Wilson, Greg and Aruliah, D. A. and Brown, C. Titus and Hong, Neil P. Chue and Davis, Matt and Guy, Richard T. and Haddock, Steven H. D. and Huff, Kathryn D. and Mitchell, Ian M. and Plumbley, Mark D. and Waugh, Ben and White, Ethan P. and Wilson, Paul},
  year = {2014},
  title = {Best Practices for Scientific Computing},
  journal = {PLoS Biology},
  issn = {1544-9173},
  doi = {10.1371/journal.pbio.1001745},
  pmid = {24415924},
  url = {http://dx.doi.org/10.1371/journal.pbio.1001745},
  volume = {12},
  pages = {e1001745},
  number = {1}
}
@article{sadedin2012bpipe,
  author = {Sadedin, Simon P. and Pope, Bernard and Oshlack, Alicia},
  year = {2012},
  title = {Bpipe: a tool for running and managing bioinformatics pipelines},
  journal = {Bioinformatics},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/bts167},
  pmid = {22500002},
  url = {http://dx.doi.org/10.1093/bioinformatics/bts167},
  volume = {28},
  month = {12},
  pages = {1525--1526},
  number = {11},
  abstract = {Summary: Bpipe is a simple, dedicated programming language for defining and executing bioinformatics pipelines. It specializes in enabling users to turn existing pipelines based on shell scripts or command line tools into highly flexible, adaptable and maintainable workflows with a minimum of effort. Bpipe ensures that pipelines execute in a controlled and repeatable fashion and keeps audit trails and logs to ensure that experimental results are reproducible. Requiring only Java as a dependency, Bpipe is fully self-contained and cross-platform, making it very easy to adopt and deploy into existing environments. Availability and implementation: Bpipe is freely available from http://bpipe.org under a BSD License. Contact: simon.sadedin{at}mcri.edu.au}
}
@article{spjuth2015experiences,
  author = {Spjuth, Ola and Bongcam-Rudloff, Erik and Hernández, Guillermo Carrasco and Forer, Lukas and Giovacchini, Mario and Guimera, Roman Valls and Kallio, Aleksi and Korpelainen, Eija and Kańduła, Maciej M and Krachunov, Milko and Kreil, David P and Kulev, Ognyan and Łabaj, Paweł P. and Lampa, Samuel and Pireddu, Luca and Schönherr, Sebastian and Siretskiy, Alexey and Vassilev, Dimitar},
  year = {2015},
  title = {Experiences with workflows for automating data-intensive bioinformatics},
  journal = {Biology Direct},
  issn = {1745-6150},
  doi = {10.1186/s13062-015-0071-8},
  pmid = {26282399},
  url = {http://dx.doi.org/10.1186/s13062-015-0071-8},
  volume = {10},
  number = {1},
  abstract = {High-throughput technologies, such as next-generation sequencing, have turned molecular biology into a data-intensive discipline, requiring bioinformaticians to use high-performance computing resources and carry out data management and analysis tasks on large scale. Workflow systems can be useful to simplify construction of analysis pipelines that automate tasks, support reproducibility and provide measures for fault-tolerance. However, workflow systems can incur significant development and administration overhead so bioinformatics pipelines are often still built without them. We present the experiences with workflows and workflow systems within the bioinformatics community participating in a series of hackathons and workshops of the EU COST action SeqAhead. The organizations are working on similar problems, but we have addressed them with different strategies and solutions. This fragmentation of efforts is inefficient and leads to redundant and incompatible solutions. Based on our experiences we define a set of recommendations for future systems to enable efficient yet simple bioinformatics workflow construction and execution. Reviewers This article was reviewed by Dr Andrew Clark.}
}
@article{cingolani2015bigdatascript,
  author = {Cingolani, Pablo and Sladek, Rob and Blanchette, Mathieu},
  year = {2015},
  title = {BigDataScript: a scripting language for data pipelines},
  journal = {Bioinformatics},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btu595},
  pmid = {25189778},
  url = {http://dx.doi.org/10.1093/bioinformatics/btu595},
  volume = {31},
  pages = {10--16},
  number = {1},
  abstract = {Motivation: The analysis of large biological datasets often requires complex processing pipelines that run for a long time on large computational infrastructures. We designed and implemented a simple script-like programming language with a clean and minimalist syntax to develop and manage pipeline execution and provide robustness to various types of software and hardware failures as well as portability. Results: We introduce the BigDataScript (BDS) programming language for data processing pipelines, which improves abstraction from hardware resources and assists with robustness. Hardware abstraction allows BDS pipelines to run without modification on a wide range of computer architectures, from a small laptop to multi-core servers, server farms, clusters and clouds. BDS achieves robustness by incorporating the concepts of absolute serialization and lazy processing, thus allowing pipelines to recover from errors. By abstracting pipeline concepts at programming language level, BDS simplifies implementation, execution and management of complex bioinformatics pipelines, resulting in reduced development and debugging cycles as well as cleaner code. Availability and implementation: BigDataScript is available under open-source license at http://pcingola.github.io/BigDataScript.}
}
@article{gafni2014cosmos,
  author = {Gafni, Erik and Luquette, Lovelace J and Lancaster, Alex K and Hawkins, Jared B and Jung, Jae-Yoon and Souilmi, Yassine and Wall, Dennis P and Tonellato, Peter J},
  year = {2014},
  title = {COSMOS: Python library for massively parallel workflows.},
  journal = {Bioinformatics (Oxford, England)},
  doi = {10.1093/bioinformatics/btu385},
  pmid = {24982428},
  url = {http://dx.doi.org/10.1093/bioinformatics/btu385},
  volume = {30},
  pages = {2956--8},
  number = {20},
  abstract = {Efficient workflows to shepherd clinically generated genomic data through the multiple stages of a next-generation sequencing pipeline are of critical importance in translational biomedical science. Here we present COSMOS, a Python library for workflow management that allows formal description of pipelines and partitioning of jobs. In addition, it includes a user interface for tracking the progress of jobs, abstraction of the queuing system and fine-grained control over the workflow. Workflows can be created on traditional computing clusters as well as cloud-based services.}
}
@article{kster2012snakemakea,
  author = {Köster, Johannes and Rahmann, Sven},
  year = {2012},
  title = {Snakemake--a scalable bioinformatics workflow engine.},
  journal = {Bioinformatics (Oxford, England)},
  pmid = {22908215},
  url = {http://www.ncbi.nlm.nih.gov/pubmed/22908215},
  volume = {28},
  month = {12},
  pages = {2520--2},
  number = {19},
  abstract = {Snakemake is a workflow engine that provides a readable Python-based workflow definition language and a powerful execution environment that scales from single-core workstations to compute clusters without modifying the workflow. It is the first system to support the use of automatically inferred multiple named wildcards (or variables) in input and output filenames.}
}
@article{goble2014better,
  author = {Goble, Carole},
  year = {2014},
  title = {Better Software, Better Research},
  journal = {IEEE Internet Computing},
  issn = {1089-7801},
  doi = {10.1109/mic.2014.88},
  url = {http://dx.doi.org/10.1109/mic.2014.88},
  volume = {18},
  pages = {4--8},
  number = {5}
}
@article{beazley2003automated,
  author = {Beazley, David M},
  year = {2003},
  title = {Automated scientific software scripting with SWIG},
  journal = {Future Generation Computer Systems},
  url = {https://scholar.google.com/scholar?cluster=14166776132178739884},
  volume = {19},
  month = {3}
}
@article{beazley1996swig,
  author = {Beazley, David M},
  year = {1996},
  title = {SWIG: An Easy to Use Tool for Integrating Scripting Languages with C and C++.},
  journal = {Tcl/Tk Workshop},
  url = {https://scholar.google.com/scholar?cluster=2768773569829356266}
}
@article{behnel2011cython,
  author = {Behnel, Stefan and Bradshaw, Robert and Citro, Craig and Dalcin, Lisandro and Seljebotn, Dag Sverre and Smith, Kurt},
  year = {2011},
  title = {Cython: The Best of Both Worlds},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2010.118},
  url = {http://dx.doi.org/10.1109/mcse.2010.118},
  volume = {13},
  month = {11},
  pages = {31--39},
  number = {2}
}
@article{hannay2009how,
  author = {Hannay, Jo Erskine and MacLeod, Carolyn and Singer, Janice and Langtangen, Hans Petter and Pfahl, Dietmar and Wilson, Greg},
  year = {2009},
  title = {How do scientists develop and use scientific software?},
  doi = {10.1109/secse.2009.5069155},
  url = {http://dx.doi.org/10.1109/secse.2009.5069155},
  month = {9},
  pages = {1--8}
}
@article{unknown2011a,
  author = {Unknown and Prabhu, Prakash and Zhang, Yun and Ghosh, Soumyadeep and August, David I and Huang, Jialu and Beard, Stephen and Kim, Hanjun and Oh, Taewook and Jablin, Thomas B and Johnson, Nick P and Zoufaly, Matthew and Raman, Arun and Liu, Feng and Walker, David},
  year = {2011},
  title = {A survey of the practice of computational science},
  doi = {10.1145/2063348.2063374},
  url = {http://dx.doi.org/10.1145/2063348.2063374},
  month = {11},
  pages = {1}
}
@article{sanders2008dealing,
  author = {Sanders, Rebecca and Kelly, Diane},
  year = {2008},
  title = {Dealing with Risk in Scientific Software Development},
  journal = {IEEE Software},
  issn = {0740-7459},
  doi = {10.1109/ms.2008.84},
  url = {http://dx.doi.org/10.1109/ms.2008.84},
  volume = {25},
  month = {8},
  pages = {21--28},
  number = {4}
}
@article{zaharia2010spark,
  author = {Zaharia, Matei and Chowdhury, Mosharaf and Franklin, Michael J and Shenker, Scott and Stoica, Ion},
  year = {2010},
  title = {Spark: cluster computing with working sets.},
  journal = {HotCloud},
  url = {https://scholar.google.com/scholar?cluster=14934743972440878947},
  volume = {10},
  month = {10}
}
@article{walt2011the,
  author = {Walt, Stefan Van Der and Colbert, S Chris and Varoquaux, Gaël},
  year = {2011},
  title = {The NumPy array: a structure for efficient numerical computation},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2011.37},
  url = {http://dx.doi.org/10.1109/mcse.2011.37},
  volume = {13},
  month = {11},
  pages = {22--30},
  number = {2},
  abstract = {In the Python world, NumPy arrays are the standard representation for numerical data. Here, we show how these arrays enable efficient implementation of numerical computations in a high-level language. Overall, three techniques are applied to improve performance: vectorizing calculations, avoiding copying data in memory, and minimizing operation counts. We first present the NumPy array structure, then show how to use it for efficient computation, and finally how to share array data with other libraries.}
}
@article{donoho2009reproducible,
  author = {Donoho, David L and Maleki, Arian and Rahman, Inam Ur and Shahram, Morteza and Stodden, Victoria},
  year = {2009},
  title = {Reproducible Research in Computational Harmonic Analysis},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2009.15},
  url = {http://dx.doi.org/10.1109/mcse.2009.15},
  volume = {11},
  month = {9},
  pages = {8--18},
  number = {1}
}
@article{leveque2012reproducible,
  author = {LeVeque, Randall J and Mitchell, Ian M and Stodden, Victoria},
  year = {2012},
  title = {Reproducible research for scientific computing: Tools and strategies for changing the culture},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2012.38},
  url = {http://dx.doi.org/10.1109/mcse.2012.38},
  volume = {14},
  month = {12},
  pages = {13--17},
  number = {4}
}
@article{fomel2015reproducible,
  author = {Fomel, Sergey},
  year = {2015},
  title = {Reproducible Research as a Community Effort: Lessons from the Madagascar Project},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2014.94},
  url = {http://dx.doi.org/10.1109/mcse.2014.94},
  volume = {17},
  pages = {20--26},
  number = {1}
}
@article{buck2015solving,
  author = {Buck, Stuart},
  year = {2015},
  title = {Solving reproducibility.},
  journal = {Science (New York, N.Y.)},
  doi = {10.1126/science.aac8041},
  pmid = {26113692},
  url = {http://dx.doi.org/10.1126/science.aac8041},
  volume = {348},
  pages = {1403},
  number = {6242}
}
@article{moreno2016improving,
  author = {Moreno, Alexander and Balch, Tucker},
  year = {2016},
  title = {Improving financial computation speed with full and subproblem memoization},
  journal = {Concurrency and Computation: Practice and Experience},
  issn = {1532-0634},
  doi = {10.1002/cpe.3693},
  url = {http://dx.doi.org/10.1002/cpe.3693},
  volume = {28},
  pages = {905--915},
  number = {3},
  abstract = {Analysts prototyping trading strategies often reuse previously computed values: both full problems and subproblems. Avoiding recomputing these would increase productivity. We built a memoization library that caches function computations to files to avoid recomputation. This should minimize the need for users to think about whether caching is appropriate while giving them control over speed, accuracy, and space usage. Guo and Engler built an automatic memoization library by modifying the Python interpreter, while jug and joblib are distributed computing libraries that do memoization. Our library attempts to maintain the ease of use of these libraries while offering a higher degree of control of how caching is carried out. It allows control of space usage for individual functions and all memoization, refreshing memoization for a specific function, and accuracy checking, and uses faster hashing and provides a divide and conquer approach to reuse previously computed subproblems. We show that for Markowitz optimization, Fama–French, and the singular value decomposition, memoization using our library greatly speeds up recomputation, often by over 99\% versus no memoization and over 80\% versus joblib. We also show how a divide-and-conquer memoization approach can give large speedups for sorting. Published 2015. This article is a U.S. Government work and is in the public domain in the USA.}
}
@article{cielik2011a,
  author = {Cieślik, Marcin and Mura, Cameron},
  year = {2011},
  title = {A lightweight, flow-based toolkit for parallel and distributed bioinformatics pipelines},
  journal = {BMC Bioinformatics},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-12-61},
  pmid = {21352538},
  url = {http://dx.doi.org/10.1186/1471-2105-12-61},
  volume = {12},
  month = {11},
  pages = {1--11},
  number = {1},
  abstract = {Background Bioinformatic analyses typically proceed as chains of data-processing tasks. A pipeline, or 'workflow', is a well-defined protocol, with a specific structure defined by the topology of data-flow interdependencies, and a particular functionality arising from the data transformations applied at each step. In computer science, the dataflow programming (DFP) paradigm defines software systems constructed in this manner, as networks of message-passing components. Thus, bioinformatic workflows can be naturally mapped onto DFP concepts. Results To enable the flexible creation and execution of bioinformatics dataflows, we have written a modular framework for parallel pipelines in Python ('PaPy'). A PaPy workflow is created from re-usable components connected by data-pipes into a directed acyclic graph, which together define nested higher-order map functions. The successive functional transformations of input data are evaluated on flexibly pooled compute resources, either local or remote. Input items are processed in batches of adjustable size, all flowing one to tune the trade-off between parallelism and lazy-evaluation (memory consumption). An add-on module ('NuBio') facilitates the creation of bioinformatics workflows by providing domain specific data-containers (e.g., for biomolecular sequences, alignments, structures) and functionality (e.g., to parse/write standard file formats). Conclusions PaPy offers a modular framework for the creation and deployment of parallel and distributed data-processing workflows. Pipelines derive their functionality from user-written, data-coupled components, so PaPy also can be viewed as a lightweight toolkit for extensible, flow-based bioinformatics data-processing. The simplicity and flexibility of distributed PaPy pipelines may help users bridge the gap between traditional desktop/workstation and grid computing. PaPy is freely distributed as open-source Python code at http://​muralab.​org/​PaPy, and includes extensive documentation and annotated usage examples.}
}
@article{pedregosa2012scikitlearn,
  author = {Pedregosa, Fabian and Varoquaux, Gaël and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, Édouard},
  year = {2012},
  title = {Scikit-learn: Machine Learning in Python},
  month = {12},
  abstract = {Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.}
}
@book{guo2011using,
  author = {Guo, Philip J. and Engler, Dawson},
  year = {2011},
  title = {Using automatic persistent memoization to facilitate data analysis scripting},
  doi = {10.1145/2001420.2001455},
  url = {http://doi.acm.org/10.1145/2001420.2001455},
  month = {11},
  pages = {287---297}
}
@article{leipzig2016a,
  author = {Leipzig, Jeremy},
  year = {2016},
  title = {A review of bioinformatic pipeline frameworks},
  journal = {Briefings in Bioinformatics},
  issn = {1467-5463},
  doi = {10.1093/bib/bbw020},
  pmid = {27013646},
  url = {http://dx.doi.org/10.1093/bib/bbw020},
  pages = {bbw020},
  abstract = {High-throughput bioinformatic analyses increasingly rely on pipeline frameworks to process sequence and metadata. Modern implementations of these frameworks differ on three key dimensions: using an implicit or explicit syntax, using a configuration, convention or class-based design paradigm and offering a command line or workbench interface. Here I survey and compare the design philosophies of several current pipeline frameworks. I provide practical recommendations based on analysis requirements and the user base.}
}
@article{celi2016datathons,
  author = {Celi, Leo Anthony and Lokhandwala, Sharukh and Montgomery, Robert and Moses, Christopher and Naumann, Tristan and Pollard, Tom and Spitz, Daniel and Stretch, Robert},
  year = {2016},
  title = {Datathons and Software to Promote Reproducible Research},
  journal = {Journal of Medical Internet Research},
  url = {https://scholar.google.com/scholar?cluster=null},
  volume = {18}
}
@article{leek2015opinion,
  author = {Leek, Jeffrey T and Peng, Roger D},
  year = {2015},
  title = {Opinion: Reproducible research can still be wrong: Adopting a prevention approach},
  journal = {Proceedings of the National Academy of Sciences},
  url = {https://scholar.google.com/scholar?cluster=16909541663760000550},
  volume = {112}
}
@article{leipzig2016a,
  author = {Leipzig, Jeremy},
  year = {2016},
  title = {A review of bioinformatic pipeline frameworks},
  journal = {Briefings in bioinformatics},
  url = {https://scholar.google.com/scholar?cluster=10431565612213639534}
}
@article{essawy2016challenges,
  author = {Essawy, Bakinam T and Goodall, Jonathan L and Malik, Tanu and Xu, Hao and Conway, Michael and Gil, Yolanda},
  year = {2016},
  title = {Challenges with Maintaining Legacy Software to Achieve Reproducible Computational Analyses: An Example for Hydrologic Modeling Data Processing Pipelines},
  url = {https://scholar.google.com/scholar?cluster=null}
}
@article{dudley2010reproducible,
  author = {Dudley, Joel T and Butte, Atul J},
  year = {2010},
  title = {Reproducible in silico research in the era of cloud computing},
  journal = {Nature biotechnology},
  url = {https://scholar.google.com/scholar?cluster=14329535853377349322},
  volume = {28},
  month = {10}
}
@article{peng2009distributed,
  author = {Peng, Roger D and Eckel, Sandrah P},
  year = {2009},
  title = {Distributed Reproducible Research Using Cached Computations},
  journal = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2009.6},
  url = {http://dx.doi.org/10.1109/mcse.2009.6},
  volume = {11},
  month = {9},
  pages = {28--34},
  number = {1}
}
@article{severin2010ehive,
  author = {Severin, Jessica and Beal, Kathryn and Vilella, Albert and Fitzgerald, Stephen and Schuster, Michael and Gordon, Leo and Ureta-Vidal, Abel and Flicek, Paul and Herrero, Javier},
  year = {2010},
  title = {eHive: An Artificial Intelligence workflow system for genomic analysis},
  journal = {BMC Bioinformatics},
  publisher = {BMC Bioinformatics},
  issn = {1471-2105},
  doi = {10.1186/1471-2105-11-240},
  url = {http://www.biomedcentral.com/1471-2105/11/240},
  volume = {11},
  month = {10},
  pages = {240},
  number = {1},
  abstract = {BACKGROUND:The Ensembl project produces updates to its comparative genomics resources with each of its several releases per year. During each release cycle approximately two weeks are allocated to generate all the genomic alignments and the protein homology predictions. The number of calculations required for this task grows approximately quadratically with the number of species. We currently support 50 species in Ensembl and we expect the number to continue to grow in the future.RESULTS:We present eHive, a new fault tolerant distributed processing system initially designed to support comparative genomic analysis, based on blackboard systems, network distributed autonomous agents, dataflow graphs and block-branch diagrams. In the eHive system a MySQL database serves as the central blackboard and the autonomous agent, a Perl script, queries the system and runs jobs as required. The system allows us to define dataflow and branching rules to suit all our production pipelines. We describe the implementation of three pipelines: (1) pairwise whole genome alignments, (2) multiple whole genome alignments and (3) gene trees with protein homology inference. Finally, we show the efficiency of the system in real case scenarios.CONCLUSIONS:eHive allows us to produce computationally demanding results in a reliable and efficient way with minimal supervision and high throughput. Further documentation is available at: http://www.ensembl.org/info/docs/eHive/ webcite.}
}
@article{mishima2011agile,
  author = {Mishima, Hiroyuki and Sasaki, Kensaku and Tanaka, Masahiro and Tatebe, Osamu and Yoshiura, Koh-ichiro},
  year = {2011},
  title = {Agile parallel bioinformatics workflow management using Pwrake},
  journal = {BMC Research Notes},
  publisher = {BMC Research Notes},
  issn = {1756-0500},
  doi = {10.1186/1756-0500-4-331},
  url = {http://www.biomedcentral.com/1756-0500/4/331},
  volume = {4},
  month = {11},
  pages = {331},
  number = {1},
  abstract = {BACKGROUND:In bioinformatics projects, scientific workflow systems are widely used to manage computational procedures. Full-featured workflow systems have been proposed to fulfil the demand for workflow management. However, such systems tend to be over-weighted for actual bioinformatics practices. We realize that quick deployment of cutting-edge software implementing advanced algorithms and data formats, and continuous adaptation to changes in computational resources and the environment are often prioritized in scientific workflow management. These features have a greater affinity with the agile software development method through iterative development phases after trial and error.Here, we show the application of a scientific workflow system Pwrake to bioinformatics workflows. Pwrake is a parallel workflow extension of Ruby's standard build tool Rake, the flexibility of which has been demonstrated in the astronomy domain. Therefore, we hypothesize that Pwrake also has advantages in actual bioinformatics workflows.FINDINGS:We implemented the Pwrake workflows to process next generation sequencing data using the Genomic Analysis Toolkit (GATK) and Dindel. GATK and Dindel workflows are typical examples of sequential and parallel workflows, respectively. We found that in practice, actual scientific workflow development iterates over two phases, the workflow definition phase and the parameter adjustment phase. We introduced separate workflow definitions to help focus on each of the two developmental phases, as well as helper methods to simplify the descriptions. This approach increased iterative development efficiency. Moreover, we implemented combined workflows to demonstrate modularity of the GATK and Dindel workflows.CONCLUSIONS:Pwrake enables agile management of scientific workflows in the bioinformatics domain. The internal domain specific language design built on Ruby gives the flexibility of rakefiles for writing scientific workflows. Furthermore, readability and maintainability of rakefiles may facilitate sharing workflows among the scientific community. Workflows for GATK and Dindel are available at http://github.com/misshie/Workflows webcite.}
}
@article{goodstadt2010ruffus,
  author = {Goodstadt, Leo},
  year = {2010},
  title = {Ruffus: A Lightweight Python Library for Computational Pipelines},
  journal = {Bioinformatics},
  publisher = {Bioinformatics},
  doi = {10.1093/bioinformatics/btq524},
  url = {http://bioinformatics.oxfordjournals.org/content/early/2010/09/16/bioinformatics.btq524.abstract},
  month = {10},
  abstract = {Summary: Computational pipelines are common place in scientific research. However, most of the resources for constructing pipelines are heavyweight systems with graphics user interfaces.Ruffus is a library for the creation of computational pipelines. Its lightweight and unobtrusive design recommends it for use even for the most trivial of analyses. At the same time, it is powerful enough to have been used for complex workflows involving more than 50 interdependent stages.Availability and Implementation: Ruffus is written in python. Source code, a short tutorial, examples, and a comprehensive user manual are freely available at http://www.ruffus.org.uk.Supplementary information: The example program is available at http://www.ruffus.org.uk/examples/bioinformaticsContact: ruffus@llew.org.uk}
}
@article{guo2010towards,
  author = {Guo, Dawson Engler Philip J.},
  year = {2010},
  title = {Towards Practical Incremental Recomputation for Scientists: An Implementation for the Python Language},
  month = {10}
}
@article{2011making,
  author = {Delescluse, Matthieu and Franconville, Romain and Joucla, Sébastien and Lieury, Tiffany and Pouzat, Christophe},
  year = {2011},
  title = {Making neurophysiological data analysis reproducible: Why and how?},
  journal = {Journal of Physiology-Paris},
  publisher = {Journal of Physiology-Paris},
  issn = {0928-4257},
  doi = {10.1016/j.jphysparis.2011.09.011},
  url = {http://www.sciencedirect.com/science/article/pii/S0928425711000374},
  month = {11},
  pages = {-- },
  number = {0}
}
@article{nordlie2009towards,
  author = {Nordlie, Eilen and Gewaltig, Marc-Oliver and Plesser, Hans Ekkehard},
  year = {2009},
  title = {Towards Reproducible Descriptions of Neuronal Network Models},
  journal = {PLoS Comput Biol},
  publisher = {PLoS Comput Biol},
  doi = {10.1371/journal.pcbi.1000456},
  url = {http://dx.doi.org/10.1371%2Fjournal.pcbi.1000456},
  volume = {5},
  month = {9},
  pages = {e1000456},
  number = {8}
}
@article{v2009reproducible,
  author = {V and ewalle, P. and Kovacevic, J. and Vetterli, M.},
  year = {2009},
  title = {Reproducible research in signal processing},
  journal = {Signal Processing Magazine, IEEE},
  publisher = {Signal Processing Magazine, IEEE},
  issn = {1053-5888},
  doi = {10.1109/msp.2009.932122},
  url = {http://dx.doi.org/10.1109/msp.2009.932122},
  volume = {26},
  month = {9},
  pages = {37 --47},
  number = {3}
}
@article{dean2008mapreduce,
  author = {Dean, Jeffrey and Ghemawat, Sanjay},
  year = {2008},
  title = {MapReduce: simplified data processing on large clusters},
  journal = {Commun. ACM},
  publisher = {Commun. ACM},
  issn = {0001-0782},
  doi = {10.1145/1327452.1327492},
  url = {http://doi.acm.org/10.1145/1327452.1327492},
  volume = {51},
  month = {8},
  pages = {107---113},
  number = {1}
}
@article{2007reproducible,
  author = {Fomel, Sergey and Hennenfent, Gilles},
  year = {2007},
  title = {Reproducible Computational Experiments using Scons},
  doi = {10.1109/icassp.2007.367305},
  url = {http://dx.doi.org/10.1109/icassp.2007.367305},
  month = {7}
}
@article{2000making,
  author = {Schwab, Matthias and Karrenbach, Martin and Claerbout, Jon},
  year = {2000},
  title = {Making Scientific Computations Reproducible},
  doi = {10.1109/5992.881708},
  url = {http://dx.doi.org/10.1109/5992.881708},
  volume = {2},
  pages = {61---67},
  number = {6}
}
@article{perez2007ipython,
  author = {Perez, Fern and o and Granger, Brian E.},
  year = {2007},
  title = {IPython: A System for Interactive Scientific Computing},
  journal = {Computing in Science \& Engineering},
  publisher = {Computing in Science \& Engineering},
  issn = {1521-9615},
  doi = {10.1109/mcse.2007.53},
  url = {http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4160251},
  volume = {9},
  month = {7},
  pages = {21---29},
  number = {3}
}
@article{coelho2013determining,
  author = {Coelho, Luis Pedro and Kangas, Joshua D and Naik, Armaghan W and Osuna-Highley, Elvira and Glory-Afshar, Estelle and Fuhrman, Margaret and Simha, Ramanuja and Berget, Peter B and Jarvik, Jonathan W and Murphy, Robert F},
  year = {2013},
  title = {Determining the subcellular location of new proteins from microscope images using local features.},
  journal = {Bioinformatics (Oxford, England)},
  issn = {1367-4803},
  doi = {10.1093/bioinformatics/btt392},
  pmid = {23836142},
  url = {http://dx.doi.org/10.1093/bioinformatics/btt392},
  volume = {29},
  pages = {2343--9},
  number = {18},
  abstract = {Evaluation of previous systems for automated determination of subcellular location from microscope images has been done using datasets in which each location class consisted of multiple images of the same representative protein. Here, we frame a more challenging and useful problem where previously unseen proteins are to be classified.}
}
